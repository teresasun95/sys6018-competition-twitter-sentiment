# factor(pred, levels=levels(y))
return(pred)
}
knn_pred('sentiment',train.clean,test.clean,k=3)
pred
library(tidyverse)
library(XML)
library(tm)
# Read in the Data
train = read.csv('train.csv', stringsAsFactors = FALSE)
test = read.csv('test.csv', stringsAsFactors = FALSE)
# create a function to clean the data
tweet.cleaner = function(X) {
get.tweets = function(X) {
tweets = data.frame(X[2], stringsAsFactors = FALSE)
tweets = VCorpus(DataframeSource(tweets))
}
tweets.clean = function(X) {
tweets.clean = tm_map(get.tweets(X), stripWhitespace)                   # remove extra whitespace
tweets.clean = tm_map(tweets.clean, removeNumbers)                      # remove numbers
tweets.clean = tm_map(tweets.clean, removePunctuation)                  # remove punctuation
tweets.clean = tm_map(tweets.clean, content_transformer(tolower))       # ignore case
tweets.clean = tm_map(tweets.clean, removeWords, stopwords("english"))  # remove stop words
tweets.clean = tm_map(tweets.clean, stemDocument)
}
return(tweets.clean(X))
}
# function to unpack a vcorpus's cleaned content into a list
tweet.unpacker = function(N) {
lapply(X = 1:length(N), function(X) {
N[[X]]$content})
}
data = data.frame(sentimentID = c(train$sentiment,test$id), text = c(train$text,test$text))
data.clean = tweet.cleaner(data)
data.tfidf = DocumentTermMatrix(data.clean, control = list(weighting = weightTfIdf))
data.b = removeSparseTerms(data.tfidf, .95) #if you want to play with the sparceness do so here, to ensure the variables remain equal
data.b.df = data.frame(as.matrix(data.b))
train.clean = data.frame(sentiment = train$sentiment, data.b.df[1:981,])
test.clean = data.frame(id = test$id, data.b.df[982:1960,])
test.clean <- test.clean[,-1]
n = nrow(test.clean)
pred <- list() # create a list to store the predictions
y <- train.clean[,'sentiment'] # get the response variable from training set
train.clean[['sentiment']]=NULL
for(i in 1:2){
dist <- apply(train.clean,1, function(x) sum((x-test.clean[i,])^2))
#print(dist)
# calculate the euclidean distance for each instance in training set
ordered <- order(dist)[1:1] # sort the list of distances
freq <- table(y[ordered]) # compute the frequencies of each class
#print(freq)
#print(names(freq))
most.frequent.classes = names(freq)[freq == max(freq)]
#print(most.frequent.classes)
pred[i] = sample(most.frequent.classes, 1)
}
pred
for(i in 1:3){
dist <- apply(train.clean,1, function(x) sum((x-test.clean[i,])^2))
#print(dist)
# calculate the euclidean distance for each instance in training set
ordered <- order(dist)[1:3] # sort the list of distances
freq <- table(y[ordered]) # compute the frequencies of each class
#print(freq)
#print(names(freq))
most.frequent.classes = names(freq)[freq == max(freq)]
#print(most.frequent.classes)
pred[i] = sample(most.frequent.classes, 1)
}
pred
return(pred)
print(pred)
pred
for(i in 1:10){
dist <- apply(train.clean,1, function(x) sum((x-test.clean[i,])^2))
#print(dist)
# calculate the euclidean distance for each instance in training set
ordered <- order(dist)[1:3] # sort the list of distances
freq <- table(y[ordered]) # compute the frequencies of each class
#print(freq)
#print(names(freq))
most.frequent.classes = names(freq)[freq == max(freq)]
#print(most.frequent.classes)
pred[i] = sample(most.frequent.classes, 1)
}
print(pred)
for(i in 1:100){
dist <- apply(train.clean,1, function(x) sum((x-test.clean[i,])^2))
#print(dist)
# calculate the euclidean distance for each instance in training set
ordered <- order(dist)[1:3] # sort the list of distances
freq <- table(y[ordered]) # compute the frequencies of each class
#print(freq)
#print(names(freq))
most.frequent.classes = names(freq)[freq == max(freq)]
#print(most.frequent.classes)
pred[i] = sample(most.frequent.classes, 1)
}
print(pred)
for(i in 1:n){
dist <- apply(train.clean,1, function(x) sum((x-test.clean[i,])^2))
#print(dist)
# calculate the euclidean distance for each instance in training set
ordered <- order(dist)[1:3] # sort the list of distances
freq <- table(y[ordered]) # compute the frequencies of each class
#print(freq)
#print(names(freq))
most.frequent.classes = names(freq)[freq == max(freq)]
#print(most.frequent.classes)
pred[i] = sample(most.frequent.classes, 1)
}
print(pred)
finalpred2<- data.frame(cbind(test$id,pred))
finalpred2
colnames(finalpred2)<- c("id", "sentiment")
df2 = as.matrix(finalpred2)
write.csv(df2, "sys6018-twitter-sentiment-KNN-k3.csv", row.names = FALSE)
library(tidyverse)
library(XML)
library(tm)
# Read in the Data
train = read.csv('train.csv', stringsAsFactors = FALSE)
test = read.csv('test.csv', stringsAsFactors = FALSE)
# create a function to clean the data
tweet.cleaner = function(X) {
get.tweets = function(X) {
tweets = data.frame(X[2], stringsAsFactors = FALSE)
tweets = VCorpus(DataframeSource(tweets))
}
tweets.clean = function(X) {
tweets.clean = tm_map(get.tweets(X), stripWhitespace)                   # remove extra whitespace
tweets.clean = tm_map(tweets.clean, removeNumbers)                      # remove numbers
tweets.clean = tm_map(tweets.clean, removePunctuation)                  # remove punctuation
tweets.clean = tm_map(tweets.clean, content_transformer(tolower))       # ignore case
tweets.clean = tm_map(tweets.clean, removeWords, stopwords("english"))  # remove stop words
tweets.clean = tm_map(tweets.clean, stemDocument)
}
return(tweets.clean(X))
}
# function to unpack a vcorpus's cleaned content into a list
tweet.unpacker = function(N) {
lapply(X = 1:length(N), function(X) {
N[[X]]$content})
}
data = data.frame(sentimentID = c(train$sentiment,test$id), text = c(train$text,test$text))
data.clean = tweet.cleaner(data)
data.tfidf = DocumentTermMatrix(data.clean, control = list(weighting = weightTfIdf))
data.b = removeSparseTerms(data.tfidf, .95) #if you want to play with the sparceness do so here, to ensure the variables remain equal
data.b.df = data.frame(as.matrix(data.b))
train.clean = data.frame(sentiment = train$sentiment, data.b.df[1:981,])
test.clean = data.frame(id = test$id, data.b.df[982:1960,])
test.clean <- test.clean[,-1]
n = nrow(test.clean)
pred <- list() # create a list to store the predictions
y <- train.clean[,'sentiment'] # get the response variable from training set
train.clean[['sentiment']]=NULL
for(i in 1:n){
dist <- apply(train.clean,1, function(x) sum((x-test.clean[i,])^2))
#print(dist)
# calculate the euclidean distance for each instance in training set
ordered <- order(dist)[1:5] # sort the list of distances
freq <- table(y[ordered]) # compute the frequencies of each class
#print(freq)
#print(names(freq))
most.frequent.classes = names(freq)[freq == max(freq)]
#print(most.frequent.classes)
pred[i] = sample(most.frequent.classes, 1)
}
pred
finalpred3<- data.frame(cbind(test$id,pred))
colnames(finalpred3)<- c("id", "sentiment")
df3 = as.matrix(finalpred3)
write.csv(df3, "sys6018-twitter-sentiment-KNN-k5.csv", row.names = FALSE)
library(tidyverse)
library(XML)
library(tm)
# Read in the Data
train = read.csv('train.csv', stringsAsFactors = FALSE)
test = read.csv('test.csv', stringsAsFactors = FALSE)
# create a function to clean the data
tweet.cleaner = function(X) {
get.tweets = function(X) {
tweets = data.frame(X[2], stringsAsFactors = FALSE)
tweets = VCorpus(DataframeSource(tweets))
}
tweets.clean = function(X) {
tweets.clean = tm_map(get.tweets(X), stripWhitespace)                   # remove extra whitespace
tweets.clean = tm_map(tweets.clean, removeNumbers)                      # remove numbers
tweets.clean = tm_map(tweets.clean, removePunctuation)                  # remove punctuation
tweets.clean = tm_map(tweets.clean, content_transformer(tolower))       # ignore case
tweets.clean = tm_map(tweets.clean, removeWords, stopwords("english"))  # remove stop words
tweets.clean = tm_map(tweets.clean, stemDocument)
}
return(tweets.clean(X))
}
# function to unpack a vcorpus's cleaned content into a list
tweet.unpacker = function(N) {
lapply(X = 1:length(N), function(X) {
N[[X]]$content})
}
data = data.frame(sentimentID = c(train$sentiment,test$id), text = c(train$text,test$text))
data.clean = tweet.cleaner(data)
data.tfidf = DocumentTermMatrix(data.clean, control = list(weighting = weightTfIdf))
data.b = removeSparseTerms(data.tfidf, .97) #if you want to play with the sparceness do so here, to ensure the variables remain equal
data.b.df = data.frame(as.matrix(data.b))
train.clean = data.frame(sentiment = train$sentiment, data.b.df[1:981,])
test.clean = data.frame(id = test$id, data.b.df[982:1960,])
test.clean <- test.clean[,-1]
n = nrow(test.clean)
pred <- list() # create a list to store the predictions
y <- train.clean[,'sentiment'] # get the response variable from training set
train.clean[['sentiment']]=NULL
for(i in 1:n){
dist <- apply(train.clean,1, function(x) sum((x-test.clean[i,])^2))
#print(dist)
# calculate the euclidean distance for each instance in training set
ordered <- order(dist)[1:3] # sort the list of distances
freq <- table(y[ordered]) # compute the frequencies of each class
#print(freq)
#print(names(freq))
most.frequent.classes = names(freq)[freq == max(freq)]
#print(most.frequent.classes)
pred[i] = sample(most.frequent.classes, 1)
}
pred
finalpred4<- data.frame(cbind(test$id,pred))
colnames(finalpred4)<- c("id", "sentiment")
df4 = as.matrix(finalpred4)
write.csv(df4, "sys6018-twitter-sentiment-KNN-k5.csv", row.names = FALSE)
write.csv(df4, "sys6018-twitter-sentiment-KNN-k3-97.csv", row.names = FALSE)
library(tidyverse)
library(XML)
library(tm)
# Read in the Data
train = read.csv('train.csv', stringsAsFactors = FALSE)
test = read.csv('test.csv', stringsAsFactors = FALSE)
# create a function to clean the data
tweet.cleaner = function(X) {
get.tweets = function(X) {
tweets = data.frame(X[2], stringsAsFactors = FALSE)
tweets = VCorpus(DataframeSource(tweets))
}
tweets.clean = function(X) {
tweets.clean = tm_map(get.tweets(X), stripWhitespace)                   # remove extra whitespace
tweets.clean = tm_map(tweets.clean, removeNumbers)                      # remove numbers
tweets.clean = tm_map(tweets.clean, removePunctuation)                  # remove punctuation
tweets.clean = tm_map(tweets.clean, content_transformer(tolower))       # ignore case
tweets.clean = tm_map(tweets.clean, removeWords, stopwords("english"))  # remove stop words
tweets.clean = tm_map(tweets.clean, stemDocument)
}
return(tweets.clean(X))
}
# function to unpack a vcorpus's cleaned content into a list
tweet.unpacker = function(N) {
lapply(X = 1:length(N), function(X) {
N[[X]]$content})
}
data = data.frame(sentimentID = c(train$sentiment,test$id), text = c(train$text,test$text))
data.clean = tweet.cleaner(data)
data.tfidf = DocumentTermMatrix(data.clean, control = list(weighting = weightTfIdf))
data.b = removeSparseTerms(data.tfidf, .97) #if you want to play with the sparceness do so here, to ensure the variables remain equal
data.b.df = data.frame(as.matrix(data.b))
train.clean = data.frame(sentiment = train$sentiment, data.b.df[1:981,])
test.clean = data.frame(id = test$id, data.b.df[982:1960,])
test.clean <- test.clean[,-1]
knn_pred <- function(response, train, test, k){
n = nrow(test)
pred <- list() # create a list to store the predictions
y <- train[,response] # get the response variable from training set
train[[response]]=NULL # remove the response variable from training set
for(i in 1:2){
dist <- apply(train,1, function(x) sum((x-test[i,])^2))
# calculate the euclidean distance for each instance in training set
ordered <- order(dist)[1:k] # sort the list of distances
freq <- table(y[ordered]) # compute the frequencies of each class
#print(freq)
#print(names(freq))
most.frequent.classes = names(freq)[freq == max(freq)]
#print(most.frequent.classes)
pred[i] = sample(most.frequent.classes, 1)
}
# factor(pred, levels=levels(y))
return(pred)
}
pred
knn_pred <- function(response, train, test, k){
n = nrow(test)
pred <- list() # create a list to store the predictions
y <- train[,response] # get the response variable from training set
train[[response]]=NULL # remove the response variable from training set
for(i in 1:3){
dist <- apply(train,1, function(x) sum((x-test[i,])^2))
# calculate the euclidean distance for each instance in training set
ordered <- order(dist)[1:k] # sort the list of distances
freq <- table(y[ordered]) # compute the frequencies of each class
#print(freq)
#print(names(freq))
most.frequent.classes = names(freq)[freq == max(freq)]
#print(most.frequent.classes)
pred[i] = sample(most.frequent.classes, 1)
}
# factor(pred, levels=levels(y))
return(pred)
}
for(i in 1:5){
dist <- apply(train,1, function(x) sum((x-test[i,])^2))
# calculate the euclidean distance for each instance in training set
ordered <- order(dist)[1:k] # sort the list of distances
freq <- table(y[ordered]) # compute the frequencies of each class
#print(freq)
#print(names(freq))
most.frequent.classes = names(freq)[freq == max(freq)]
#print(most.frequent.classes)
pred[i] = sample(most.frequent.classes, 1)
}
for(i in 1:2){
dist <- apply(train,1, function(x) sum((x-test[i,])^2))
# calculate the euclidean distance for each instance in training set
ordered <- order(dist)[1:k] # sort the list of distances
freq <- table(y[ordered]) # compute the frequencies of each class
#print(freq)
#print(names(freq))
most.frequent.classes = names(freq)[freq == max(freq)]
#print(most.frequent.classes)
pred[i] = sample(most.frequent.classes, 1)
}
library(tidyverse)
library(XML)
library(tm)
# Read in the Data
train = read.csv('train.csv', stringsAsFactors = FALSE)
test = read.csv('test.csv', stringsAsFactors = FALSE)
# create a function to clean the data
tweet.cleaner = function(X) {
get.tweets = function(X) {
tweets = data.frame(X[2], stringsAsFactors = FALSE)
tweets = VCorpus(DataframeSource(tweets))
}
tweets.clean = function(X) {
tweets.clean = tm_map(get.tweets(X), stripWhitespace)                   # remove extra whitespace
tweets.clean = tm_map(tweets.clean, removeNumbers)                      # remove numbers
tweets.clean = tm_map(tweets.clean, removePunctuation)                  # remove punctuation
tweets.clean = tm_map(tweets.clean, content_transformer(tolower))       # ignore case
tweets.clean = tm_map(tweets.clean, removeWords, stopwords("english"))  # remove stop words
tweets.clean = tm_map(tweets.clean, stemDocument)
}
return(tweets.clean(X))
}
# function to unpack a vcorpus's cleaned content into a list
tweet.unpacker = function(N) {
lapply(X = 1:length(N), function(X) {
N[[X]]$content})
}
data = data.frame(sentimentID = c(train$sentiment,test$id), text = c(train$text,test$text))
data.clean = tweet.cleaner(data)
data.tfidf = DocumentTermMatrix(data.clean, control = list(weighting = weightTfIdf))
data.b = removeSparseTerms(data.tfidf, .97) #if you want to play with the sparceness do so here, to ensure the variables remain equal
data.b.df = data.frame(as.matrix(data.b))
train.clean = data.frame(sentiment = train$sentiment, data.b.df[1:981,])
test.clean = data.frame(id = test$id, data.b.df[982:1960,])
test.clean <- test.clean[,-1]
knn_pred <- function(response, train, test, k){
n = nrow(test)
pred <- list() # create a list to store the predictions
y <- train[,response] # get the response variable from training set
train[[response]]=NULL # remove the response variable from training set
for(i in 1:5){
dist <- apply(train,1, function(x) sum((x-test[i,])^2))
# calculate the euclidean distance for each instance in training set
ordered <- order(dist)[1:k] # sort the list of distances
freq <- table(y[ordered]) # compute the frequencies of each class
#print(freq)
#print(names(freq))
most.frequent.classes = names(freq)[freq == max(freq)]
#print(most.frequent.classes)
pred[i] = sample(most.frequent.classes, 1)
}
# factor(pred, levels=levels(y))
return(pred)
}
pred
knn_pred <- function(response, train, test, k){
n = nrow(test)
pred <- list() # create a list to store the predictions
y <- train[,response] # get the response variable from training set
train[[response]]=NULL # remove the response variable from training set
for(i in 1:5){
dist <- apply(train,1, function(x) sum((x-test[i,])^2))
# calculate the euclidean distance for each instance in training set
ordered <- order(dist)[1:k] # sort the list of distances
freq <- table(y[ordered]) # compute the frequencies of each class
#print(freq)
#print(names(freq))
most.frequent.classes = names(freq)[freq == max(freq)]
#print(most.frequent.classes)
pred[i] = sample(most.frequent.classes, 1)
}
# factor(pred, levels=levels(y))
print(pred)
}
knn_pred <- function(response, train, test, k){
n = nrow(test)
pred <- list() # create a list to store the predictions
y <- train[,response] # get the response variable from training set
train[[response]]=NULL # remove the response variable from training set
for(i in 1:5){
dist <- apply(train,1, function(x) sum((x-test[i,])^2))
# calculate the euclidean distance for each instance in training set
ordered <- order(dist)[1:k] # sort the list of distances
freq <- table(y[ordered]) # compute the frequencies of each class
#print(freq)
#print(names(freq))
most.frequent.classes = names(freq)[freq == max(freq)]
#print(most.frequent.classes)
pred[i] = sample(most.frequent.classes, 1)
}
# factor(pred, levels=levels(y))
pred
}
knn_pred <- function(response, train, test, k){
n = nrow(test)
pred <- list() # create a list to store the predictions
y <- train[,response] # get the response variable from training set
train[[response]]=NULL # remove the response variable from training set
for(i in 1:5){
dist <- apply(train,1, function(x) sum((x-test[i,])^2))
# calculate the euclidean distance for each instance in training set
ordered <- order(dist)[1:k] # sort the list of distances
freq <- table(y[ordered]) # compute the frequencies of each class
#print(freq)
#print(names(freq))
most.frequent.classes = names(freq)[freq == max(freq)]
#print(most.frequent.classes)
pred[i] = sample(most.frequent.classes, 1)
}
# factor(pred, levels=levels(y))
return(pred)
}
library(tidyverse)
library(XML)
library(tm)
# Read in the Data
train = read.csv('train.csv', stringsAsFactors = FALSE)
test = read.csv('test.csv', stringsAsFactors = FALSE)
# create a function to clean the data
tweet.cleaner = function(X) {
get.tweets = function(X) {
tweets = data.frame(X[2], stringsAsFactors = FALSE)
tweets = VCorpus(DataframeSource(tweets))
}
tweets.clean = function(X) {
tweets.clean = tm_map(get.tweets(X), stripWhitespace)                   # remove extra whitespace
tweets.clean = tm_map(tweets.clean, removeNumbers)                      # remove numbers
tweets.clean = tm_map(tweets.clean, removePunctuation)                  # remove punctuation
tweets.clean = tm_map(tweets.clean, content_transformer(tolower))       # ignore case
tweets.clean = tm_map(tweets.clean, removeWords, stopwords("english"))  # remove stop words
tweets.clean = tm_map(tweets.clean, stemDocument)
}
return(tweets.clean(X))
}
# function to unpack a vcorpus's cleaned content into a list
tweet.unpacker = function(N) {
lapply(X = 1:length(N), function(X) {
N[[X]]$content})
}
data = data.frame(sentimentID = c(train$sentiment,test$id), text = c(train$text,test$text))
data.clean = tweet.cleaner(data)
data.tfidf = DocumentTermMatrix(data.clean, control = list(weighting = weightTfIdf))
data.b = removeSparseTerms(data.tfidf, .97) #if you want to play with the sparceness do so here, to ensure the variables remain equal
data.b.df = data.frame(as.matrix(data.b))
train.clean = data.frame(sentiment = train$sentiment, data.b.df[1:981,])
test.clean = data.frame(id = test$id, data.b.df[982:1960,])
test.clean <- test.clean[,-1]
knn_pred <- function(response, train, test, k){
n = nrow(test)
pred <- list() # create a list to store the predictions
y <- train[,response] # get the response variable from training set
train[[response]]=NULL # remove the response variable from training set
for(i in 1:5){
dist <- apply(train,1, function(x) sum((x-test[i,])^2))
# calculate the euclidean distance for each instance in training set
ordered <- order(dist)[1:k] # sort the list of distances
freq <- table(y[ordered]) # compute the frequencies of each class
#print(freq)
#print(names(freq))
most.frequent.classes = names(freq)[freq == max(freq)]
#print(most.frequent.classes)
pred[i] = sample(most.frequent.classes, 1)
}
# factor(pred, levels=levels(y))
return(pred)
}
knn_pred <- function(response, train, test, k){
n = nrow(test)
pred <- list() # create a list to store the predictions
y <- train[,response] # get the response variable from training set
train[[response]]=NULL # remove the response variable from training set
for(i in 1:2){
dist <- apply(train,1, function(x) sum((x-test[i,])^2))
# calculate the euclidean distance for each instance in training set
ordered <- order(dist)[1:k] # sort the list of distances
freq <- table(y[ordered]) # compute the frequencies of each class
#print(freq)
#print(names(freq))
most.frequent.classes = names(freq)[freq == max(freq)]
#print(most.frequent.classes)
pred[i] = sample(most.frequent.classes, 1)
}
# factor(pred, levels=levels(y))
return(pred)
}
pred
